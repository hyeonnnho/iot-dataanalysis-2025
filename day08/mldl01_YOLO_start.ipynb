{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f465724",
   "metadata": {},
   "source": [
    "## YOLO\n",
    "\n",
    "### PyTorch 기반 물체인식 모델\n",
    "- CNN, rCNN(Regions with CNN)\n",
    "\n",
    "#### YOLOv5 이상 설치\n",
    "```shell\n",
    "> pip install ultralytics\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a915fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (8.3.109)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (3.10.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (1.15.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (0.21.0+cu118)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.13.1)\n",
      "Requirement already satisfied: networkx in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec62ff",
   "metadata": {},
   "source": [
    "#### 콘솔에서 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb9e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.109 🚀 Python-3.11.9 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "image 1/1 c:\\Source\\iot-dataanalysis-2025\\day08\\bus.jpg: 640x480 4 persons, 1 bus, 30.6ms\n",
      "Speed: 2.8ms preprocess, 30.6ms inference, 59.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mC:\\Source\\iot-dataanalysis-2025\\runs\\detect\\predict2\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# 콘솔에서 예측\n",
    "## yolo11n.pt : pretrained YOLO model\n",
    "## 자동으로 yolo11n.pt 다운로드\n",
    "## 웹 URL에 있는 이미지도 예측이 가능\n",
    "!yolo predict model = yolo11n.pt source='./bus.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befaa036",
   "metadata": {},
   "source": [
    "#### 파이썬으로 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc929d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO 모듈 로드\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758d69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO 클래스가 들어오는 모델의 버전에 따라서 알아서 YOLO 예측모델 객체 생성\n",
    "model = YOLO('./yolo11n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0659febe",
   "metadata": {},
   "source": [
    "##### coco8.yaml\n",
    "- https://github.com/ultralytics/assets/releases/download/v0.0.0/coco8.zip\n",
    "- 위 내용대로 훈련을 시킨 결과 -> yolo11n.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f7092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.109  Python-3.11.9 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=./yolo11n.pt, data=./coco8.yaml, epochs=100, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda:0, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=C:\\Source\\iot-dataanalysis-2025\\runs\\detect\\train\n",
      "\n",
      "Dataset 'coco8.yaml' images not found , missing path 'C:\\Source\\datasets\\coco8\\images\\val'\n",
      "Downloading https://ultralytics.com/assets/coco8.zip to 'C:\\Source\\datasets\\coco8.zip'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 433k/433k [00:00<00:00, 23.5MB/s]\n",
      "Unzipping C:\\Source\\datasets\\coco8.zip to C:\\Source\\datasets\\coco8...: 100%|██████████| 25/25 [00:00<00:00, 1083.21file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset download success  (0.9s), saved to \u001b[1mC:\\Source\\datasets\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ultralytics.com/assets/Arial.ttf to 'C:\\Users\\Admin\\AppData\\Roaming\\Ultralytics\\Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 755k/755k [00:00<00:00, 26.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Source\\iot-dataanalysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    464912  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLO11n summary: 181 layers, 2,624,080 parameters, 2,624,064 gradients, 6.6 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir C:\\Source\\iot-dataanalysis-2025\\runs\\detect\\train', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks failed . AMP training on NVIDIA GeForce GTX 1650 GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Source\\datasets\\coco8\\labels\\train... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<00:00, 218.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Source\\datasets\\coco8\\labels\\train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Source\\datasets\\coco8\\labels\\val... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<00:00, 3403.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Source\\datasets\\coco8\\labels\\val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Source\\iot-dataanalysis-2025\\runs\\detect\\train\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100      1.23G      1.096      2.751      1.478         21        640: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.566       0.85      0.849      0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/100      1.26G      1.165      2.778      1.475         36        640: 100%|██████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.557       0.85      0.887       0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/100      1.27G       1.09      2.487      1.215         20        640: 100%|██████████| 1/1 [00:00<00:00,  3.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.558       0.85       0.85      0.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/100      1.29G       1.17      2.767      1.486         21        640: 100%|██████████| 1/1 [00:00<00:00,  4.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 11.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17       0.55       0.85      0.857      0.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/100       1.3G      1.198      2.397      1.608         19        640: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 11.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.546       0.85      0.859      0.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      6/100      1.31G     0.8209      1.833      1.141         22        640: 100%|██████████| 1/1 [00:00<00:00,  4.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 11.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.551       0.86      0.858      0.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      7/100      1.34G      1.403      2.689      1.757         20        640: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 11.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.544      0.867      0.861      0.629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      8/100      1.35G      1.232       3.77      1.674         20        640: 100%|██████████| 1/1 [00:00<00:00,  4.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.548      0.875      0.898      0.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      9/100      1.36G     0.8251      2.752      1.235         20        640: 100%|██████████| 1/1 [00:00<00:00,  4.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# coco8.yaml - YOLO 훈련에 사용할 데이터셋 정의 파일\n",
    "train_results = model.train(\n",
    "    data='./coco8.yaml',\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    device='cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b3e00a",
   "metadata": {},
   "source": [
    "#### 이미지 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e809e5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Source\\iot-dataanalysis-2025\\day08\\0000001.jpg: 480x640 1 cat, 30.5ms\n",
      "Speed: 1.3ms preprocess, 30.5ms inference, 60.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "result = model('./0000001.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76248971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib 모듈 로드\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d650102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측결과 이미지 저장\n",
    "img = result[0].plot()\n",
    "img_pil = Image.fromarray(img[..., ::-1])\n",
    "img_pil.save('./predict_result.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9ab6c",
   "metadata": {},
   "source": [
    "#### OpenCV\n",
    "- Opensource Computer Vision 약자. 실시간 컴퓨터 비전(시각처리)을 목적으로 프로그래밍 라이브러리\n",
    "- 인텔에서 2000년에 C, C++ 사용하기 위해서 개발\n",
    "- 파이썬에서 사용할 수 있게 래핑\n",
    "\n",
    "```shell\n",
    "> pip install opencv-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02412762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "# OpenCV 설치\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d403262e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.11.0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbfded35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464, 640, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2 = cv2.imread('./predict_result.jpg')\n",
    "img2.shape # (464, 640, 3)-> (height, width, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 윈도우 창 오픈\n",
    "cv2.imshow('predict_result', img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abcf0e",
   "metadata": {},
   "source": [
    "#### YOLO 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e05c8f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 416x640 1 bottle, 1 cup, 1 bowl, 1 tv, 1 mouse, 1 book, 39.3ms\n",
      "Speed: 2.2ms preprocess, 39.3ms inference, 2.7ms postprocess per image at shape (1, 3, 416, 640)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread('0000002.jpg')\n",
    "resized_img = cv2.resize(img, (640, 400))\n",
    "\n",
    "result = model(resized_img)\n",
    "plots = result[0].plot()\n",
    "\n",
    "cv2.imshow('predict_result', plots)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a42f83",
   "metadata": {},
   "source": [
    "#### 동영상 플레이\n",
    "- 라즈베리파이에서 동일하게 사용가능\n",
    "- 라즈베리파이 웹캠 사용추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36dce621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비디오 파일 경로\n",
    "video_path = './sample01.mp4'\n",
    "output_path = './sample01_output.mp4'\n",
    "count_path = './sample01_count.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e11768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동영상 플레이\n",
    "cap = cv2.VideoCapture(video_path) # 0 -> 웹캠이나 카메라 설치된 번호\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    cv2.imshow('Video play', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # q버튼을 누르면\n",
    "        break\n",
    "cap.release() # 비디오를 해제\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f6c6e",
   "metadata": {},
   "source": [
    "#### YOLO 실시간 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3468cc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 모듈 로드\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7e9690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 train, 157.5ms\n",
      "Speed: 3.3ms preprocess, 157.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 train, 10.6ms\n",
      "Speed: 2.0ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 train, 11.9ms\n",
      "Speed: 1.3ms preprocess, 11.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 train, 10.4ms\n",
      "Speed: 1.8ms preprocess, 10.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 train, 9.9ms\n",
      "Speed: 2.2ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 train, 10.5ms\n",
      "Speed: 2.1ms preprocess, 10.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 train, 11.2ms\n",
      "Speed: 1.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 train, 11.2ms\n",
      "Speed: 1.3ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 10.5ms\n",
      "Speed: 1.5ms preprocess, 10.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 10.7ms\n",
      "Speed: 1.6ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 10.2ms\n",
      "Speed: 1.9ms preprocess, 10.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 10.2ms\n",
      "Speed: 1.4ms preprocess, 10.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 train, 10.2ms\n",
      "Speed: 1.5ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 train, 9.9ms\n",
      "Speed: 1.8ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 11.0ms\n",
      "Speed: 1.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 train, 11.3ms\n",
      "Speed: 1.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 10.2ms\n",
      "Speed: 1.3ms preprocess, 10.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 10.5ms\n",
      "Speed: 1.5ms preprocess, 10.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 9.7ms\n",
      "Speed: 1.8ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 10.5ms\n",
      "Speed: 1.3ms preprocess, 10.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 11.0ms\n",
      "Speed: 1.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 10.7ms\n",
      "Speed: 1.4ms preprocess, 10.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 10.6ms\n",
      "Speed: 1.3ms preprocess, 10.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 10.1ms\n",
      "Speed: 1.5ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 9.9ms\n",
      "Speed: 1.8ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.1ms\n",
      "Speed: 1.8ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 10.1ms\n",
      "Speed: 2.4ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.6ms\n",
      "Speed: 1.3ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.4ms\n",
      "Speed: 1.3ms preprocess, 10.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.8ms\n",
      "Speed: 1.3ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.1ms\n",
      "Speed: 1.4ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 11.3ms\n",
      "Speed: 1.6ms preprocess, 11.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 10.2ms\n",
      "Speed: 2.5ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 10.6ms\n",
      "Speed: 2.4ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 11.1ms\n",
      "Speed: 1.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 11.9ms\n",
      "Speed: 1.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 10.3ms\n",
      "Speed: 1.4ms preprocess, 10.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 10.8ms\n",
      "Speed: 1.2ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 10.1ms\n",
      "Speed: 1.5ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 9.7ms\n",
      "Speed: 1.9ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 9.9ms\n",
      "Speed: 1.3ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 9.8ms\n",
      "Speed: 1.6ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.4ms\n",
      "Speed: 1.3ms preprocess, 10.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 9.9ms\n",
      "Speed: 1.2ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 9.7ms\n",
      "Speed: 1.3ms preprocess, 9.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 10.1ms\n",
      "Speed: 1.3ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 12.3ms\n",
      "Speed: 1.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 19.5ms\n",
      "Speed: 1.7ms preprocess, 19.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 25.1ms\n",
      "Speed: 1.9ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 21.9ms\n",
      "Speed: 1.8ms preprocess, 21.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 23.1ms\n",
      "Speed: 2.0ms preprocess, 23.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 21.1ms\n",
      "Speed: 1.8ms preprocess, 21.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.6ms\n",
      "Speed: 1.3ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 10.6ms\n",
      "Speed: 1.4ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 10.1ms\n",
      "Speed: 1.3ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 10.8ms\n",
      "Speed: 1.3ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 12.4ms\n",
      "Speed: 2.2ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 10.7ms\n",
      "Speed: 2.4ms preprocess, 10.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 11.1ms\n",
      "Speed: 1.3ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 9.7ms\n",
      "Speed: 1.7ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.0ms\n",
      "Speed: 1.3ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 11.3ms\n",
      "Speed: 1.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 10.4ms\n",
      "Speed: 1.2ms preprocess, 10.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 9.9ms\n",
      "Speed: 1.4ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 9.6ms\n",
      "Speed: 2.1ms preprocess, 9.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 10.6ms\n",
      "Speed: 1.3ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 10.0ms\n",
      "Speed: 1.3ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 9.7ms\n",
      "Speed: 1.7ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 11.2ms\n",
      "Speed: 1.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 11.0ms\n",
      "Speed: 1.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.5ms\n",
      "Speed: 1.3ms preprocess, 10.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 10.2ms\n",
      "Speed: 1.3ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 2 trains, 9.9ms\n",
      "Speed: 1.3ms preprocess, 9.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 9.9ms\n",
      "Speed: 1.6ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 10.1ms\n",
      "Speed: 1.7ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.2ms\n",
      "Speed: 1.4ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.2ms\n",
      "Speed: 1.2ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 11.1ms\n",
      "Speed: 1.6ms preprocess, 11.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.0ms\n",
      "Speed: 1.6ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.4ms\n",
      "Speed: 1.9ms preprocess, 10.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.1ms\n",
      "Speed: 1.7ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 11.3ms\n",
      "Speed: 1.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 11.3ms\n",
      "Speed: 1.4ms preprocess, 11.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 9.8ms\n",
      "Speed: 1.4ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 12.9ms\n",
      "Speed: 2.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 9.7ms\n",
      "Speed: 1.4ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.7ms\n",
      "Speed: 1.8ms preprocess, 10.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 toilet, 11.8ms\n",
      "Speed: 1.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.1ms\n",
      "Speed: 1.4ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 10.3ms\n",
      "Speed: 1.4ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 10.2ms\n",
      "Speed: 1.8ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 10.3ms\n",
      "Speed: 1.4ms preprocess, 10.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 10.2ms\n",
      "Speed: 1.3ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.2ms\n",
      "Speed: 1.3ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 9.9ms\n",
      "Speed: 1.4ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 19.9ms\n",
      "Speed: 1.7ms preprocess, 19.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 22.7ms\n",
      "Speed: 1.9ms preprocess, 22.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 22.0ms\n",
      "Speed: 3.5ms preprocess, 22.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 22.0ms\n",
      "Speed: 1.8ms preprocess, 22.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 13.1ms\n",
      "Speed: 2.5ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 12.6ms\n",
      "Speed: 1.5ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.5ms\n",
      "Speed: 1.4ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 10.3ms\n",
      "Speed: 1.4ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 9.5ms\n",
      "Speed: 1.5ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 9.7ms\n",
      "Speed: 2.6ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 10.8ms\n",
      "Speed: 1.4ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 10.0ms\n",
      "Speed: 1.3ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 10.8ms\n",
      "Speed: 1.6ms preprocess, 10.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 10.9ms\n",
      "Speed: 1.4ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 10.1ms\n",
      "Speed: 1.5ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 10.0ms\n",
      "Speed: 1.9ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 9.5ms\n",
      "Speed: 1.4ms preprocess, 9.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 11.8ms\n",
      "Speed: 3.1ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 1 truck, 10.5ms\n",
      "Speed: 1.4ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 1 truck, 10.0ms\n",
      "Speed: 1.4ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 10.0ms\n",
      "Speed: 1.8ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.1ms\n",
      "Speed: 2.2ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.1ms\n",
      "Speed: 1.5ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 10.4ms\n",
      "Speed: 1.3ms preprocess, 10.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 10.2ms\n",
      "Speed: 1.4ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 1 traffic light, 10.8ms\n",
      "Speed: 2.0ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 10.0ms\n",
      "Speed: 1.4ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 11.8ms\n",
      "Speed: 1.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 10.3ms\n",
      "Speed: 1.3ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 10.2ms\n",
      "Speed: 1.8ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 12.4ms\n",
      "Speed: 1.3ms preprocess, 12.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 10.9ms\n",
      "Speed: 1.4ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 10.3ms\n",
      "Speed: 1.3ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 10.1ms\n",
      "Speed: 1.2ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 train, 9.5ms\n",
      "Speed: 1.6ms preprocess, 9.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 train, 11.1ms\n",
      "Speed: 1.5ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 train, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 train, 10.1ms\n",
      "Speed: 1.4ms preprocess, 10.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 train, 9.7ms\n",
      "Speed: 2.5ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 train, 9.9ms\n",
      "Speed: 1.4ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 train, 10.0ms\n",
      "Speed: 1.3ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 train, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 train, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 train, 11.0ms\n",
      "Speed: 1.3ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 train, 10.5ms\n",
      "Speed: 1.4ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 10.5ms\n",
      "Speed: 1.4ms preprocess, 10.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 10.1ms\n",
      "Speed: 1.3ms preprocess, 10.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 train, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.1ms\n",
      "Speed: 1.5ms preprocess, 10.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.6ms\n",
      "Speed: 1.5ms preprocess, 10.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 9.7ms\n",
      "Speed: 1.5ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 9.9ms\n",
      "Speed: 2.0ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 12.6ms\n",
      "Speed: 1.3ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 10.2ms\n",
      "Speed: 1.3ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 12.2ms\n",
      "Speed: 1.3ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 train, 10.5ms\n",
      "Speed: 1.2ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 train, 11.9ms\n",
      "Speed: 1.2ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.4ms\n",
      "Speed: 1.4ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 10.2ms\n",
      "Speed: 1.3ms preprocess, 10.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 10.1ms\n",
      "Speed: 1.4ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 10.2ms\n",
      "Speed: 1.3ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 10.5ms\n",
      "Speed: 1.6ms preprocess, 10.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 11.7ms\n",
      "Speed: 1.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 11.8ms\n",
      "Speed: 1.3ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 10.4ms\n",
      "Speed: 1.3ms preprocess, 10.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 10.1ms\n",
      "Speed: 1.7ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 11.4ms\n",
      "Speed: 1.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 9.9ms\n",
      "Speed: 1.4ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 10.2ms\n",
      "Speed: 1.2ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 10.2ms\n",
      "Speed: 1.3ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 9.3ms\n",
      "Speed: 1.4ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 10.2ms\n",
      "Speed: 1.6ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 10.7ms\n",
      "Speed: 1.5ms preprocess, 10.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 10.0ms\n",
      "Speed: 1.4ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 14.0ms\n",
      "Speed: 1.7ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 11.2ms\n",
      "Speed: 1.9ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 9.4ms\n",
      "Speed: 1.9ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 9.7ms\n",
      "Speed: 1.7ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 10.2ms\n",
      "Speed: 1.2ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 10.1ms\n",
      "Speed: 1.3ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 10.4ms\n",
      "Speed: 1.3ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 10.7ms\n",
      "Speed: 1.3ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 9.3ms\n",
      "Speed: 1.8ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 9.2ms\n",
      "Speed: 1.7ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 10.0ms\n",
      "Speed: 1.5ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 12.5ms\n",
      "Speed: 1.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 10.1ms\n",
      "Speed: 1.4ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 9.9ms\n",
      "Speed: 1.6ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 10.4ms\n",
      "Speed: 1.4ms preprocess, 10.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 9.9ms\n",
      "Speed: 1.5ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 9.9ms\n",
      "Speed: 1.7ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 10.0ms\n",
      "Speed: 1.5ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 10.0ms\n",
      "Speed: 2.5ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 10.0ms\n",
      "Speed: 1.3ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 10.3ms\n",
      "Speed: 1.3ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 10.3ms\n",
      "Speed: 1.4ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 9.7ms\n",
      "Speed: 1.5ms preprocess, 9.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 9.4ms\n",
      "Speed: 1.5ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 10.7ms\n",
      "Speed: 1.7ms preprocess, 10.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 11.3ms\n",
      "Speed: 1.7ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 9.7ms\n",
      "Speed: 1.7ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 1 truck, 10.8ms\n",
      "Speed: 1.4ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 12.5ms\n",
      "Speed: 2.1ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 10.2ms\n",
      "Speed: 1.3ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 11.2ms\n",
      "Speed: 1.3ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 9.8ms\n",
      "Speed: 1.4ms preprocess, 9.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 9.4ms\n",
      "Speed: 1.9ms preprocess, 9.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 suitcase, 9.6ms\n",
      "Speed: 2.1ms preprocess, 9.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 12.0ms\n",
      "Speed: 1.7ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 9.9ms\n",
      "Speed: 1.4ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 10.1ms\n",
      "Speed: 1.5ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 12.2ms\n",
      "Speed: 1.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 10.7ms\n",
      "Speed: 1.3ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 9.7ms\n",
      "Speed: 1.5ms preprocess, 9.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 11.2ms\n",
      "Speed: 1.7ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 10.6ms\n",
      "Speed: 1.3ms preprocess, 10.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 truck, 11.0ms\n",
      "Speed: 1.5ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 10.1ms\n",
      "Speed: 2.1ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 11.5ms\n",
      "Speed: 1.2ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 10.3ms\n",
      "Speed: 1.5ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 9.8ms\n",
      "Speed: 1.3ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 12.3ms\n",
      "Speed: 1.5ms preprocess, 12.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 9.8ms\n",
      "Speed: 1.3ms preprocess, 9.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 9.3ms\n",
      "Speed: 1.3ms preprocess, 9.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 10.5ms\n",
      "Speed: 1.4ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 12.1ms\n",
      "Speed: 1.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 11.8ms\n",
      "Speed: 1.2ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 11.2ms\n",
      "Speed: 1.3ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 10.2ms\n",
      "Speed: 1.3ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 12.0ms\n",
      "Speed: 1.3ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 10.0ms\n",
      "Speed: 1.3ms preprocess, 10.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 11.1ms\n",
      "Speed: 1.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 12.1ms\n",
      "Speed: 1.8ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 10.0ms\n",
      "Speed: 1.3ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 11.7ms\n",
      "Speed: 1.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.0ms\n",
      "Speed: 1.7ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.4ms\n",
      "Speed: 1.3ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 12.2ms\n",
      "Speed: 1.4ms preprocess, 12.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 9.6ms\n",
      "Speed: 2.1ms preprocess, 9.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 9.9ms\n",
      "Speed: 1.5ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.2ms\n",
      "Speed: 1.5ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 12.1ms\n",
      "Speed: 1.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 9.3ms\n",
      "Speed: 1.4ms preprocess, 9.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 10.6ms\n",
      "Speed: 1.9ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 12.0ms\n",
      "Speed: 2.1ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 11.6ms\n",
      "Speed: 1.3ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 11.0ms\n",
      "Speed: 1.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 12.6ms\n",
      "Speed: 1.3ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 9.3ms\n",
      "Speed: 1.4ms preprocess, 9.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 11.3ms\n",
      "Speed: 3.2ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 12.3ms\n",
      "Speed: 1.3ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 10.7ms\n",
      "Speed: 1.3ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 9.8ms\n",
      "Speed: 1.4ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 11.7ms\n",
      "Speed: 2.0ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 11.6ms\n",
      "Speed: 1.2ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.4ms\n",
      "Speed: 1.3ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.0ms\n",
      "Speed: 1.4ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 12.5ms\n",
      "Speed: 1.6ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 9.8ms\n",
      "Speed: 2.3ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.8ms\n",
      "Speed: 1.3ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 11.0ms\n",
      "Speed: 1.3ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.7ms\n",
      "Speed: 1.3ms preprocess, 10.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 11.4ms\n",
      "Speed: 1.7ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.7ms\n",
      "Speed: 1.3ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 10.6ms\n",
      "Speed: 1.5ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # 동영상 FPS(Frame Per Second)\n",
    "frame_time = 1.0/fps # 초단위로 변환\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))      # 1280\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))    # 720\n",
    "\n",
    "# VideoWriter 객체 생성(동영상 화면에 그림, 글자를 그리기 위한 객체)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    start_time = time.time() # 시작시간\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    # 객체 탐지\n",
    "    results = model(frame)\n",
    "    # 탐지 결과 그리기\n",
    "    for result in results:\n",
    "        detect_frame = result.plot()\n",
    "    # 결과 프레임을 파일로 저장\n",
    "    out.write(detect_frame)\n",
    "    # 결과 표시\n",
    "    cv2.imshow('YOLO Object Detection', detect_frame)\n",
    "    cv2.imshow('Video play', frame)\n",
    "\n",
    "    # 프레임간 실제 지연시간 계산\n",
    "    elapsed_time = time.time() - start_time\n",
    "    delay = max(int((frame_time - elapsed_time) * 1000), 1)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # q버튼을 누르면\n",
    "        break\n",
    "cap.release() # 비디오를 해제\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e002dab",
   "metadata": {},
   "source": [
    "#### Car Counting\n",
    "- 지정된 라인 아래로 내려오는 자동차 개수 카운팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35044588",
   "metadata": {},
   "source": [
    "- shapely 설치\n",
    "```shell\n",
    "> pip install shapely==2.0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded6898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shapely==2.0.1\n",
      "  Downloading shapely-2.0.1-cp311-cp311-win_amd64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\source\\iot-dataanalysis-2025\\mlvenv\\lib\\site-packages (from shapely==2.0.1) (1.26.4)\n",
      "Downloading shapely-2.0.1-cp311-cp311-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 11.7 MB/s eta 0:00:00\n",
      "Installing collected packages: shapely\n",
      "Successfully installed shapely-2.0.1\n"
     ]
    }
   ],
   "source": [
    "# shapely 설치\n",
    "!pip install shapely==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37bbbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics.solutions import ObjectCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210bb6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics Solutions:  {'region': [(20, 400), (1080, 400)], 'show_in': True, 'show_out': True, 'colormap': None, 'up_angle': 145.0, 'down_angle': 90, 'kpts': [6, 8, 10], 'analytics_type': 'line', 'json_file': None, 'records': 5, 'show': True, 'model': 'yolo11n.pt'}\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lap'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Source\\iot-dataanalysis-2025\\mlvenv\\Lib\\site-packages\\ultralytics\\trackers\\utils\\matching.py:10\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlap\u001b[39;00m  \u001b[38;5;66;03m# for linear_assignment\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m lap.__version__  \u001b[38;5;66;03m# verify package is not directory\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lap'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     ret, frame = cap.read()\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     results = \u001b[43mcounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     out.write(results.plot_im)\n\u001b[32m     28\u001b[39m cap.release() \u001b[38;5;66;03m# 비디오를 해제\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Source\\iot-dataanalysis-2025\\mlvenv\\Lib\\site-packages\\ultralytics\\solutions\\solutions.py:195\u001b[39m, in \u001b[36mBaseSolution.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    194\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Allow instances to be called like a function with flexible arguments.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call the subclass-specific process method\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.CFG[\u001b[33m\"\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m\"\u001b[39m]:  \u001b[38;5;66;03m# extract verbose value to display the output logs if True\u001b[39;00m\n\u001b[32m    197\u001b[39m         LOGGER.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🚀 Results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Source\\iot-dataanalysis-2025\\mlvenv\\Lib\\site-packages\\ultralytics\\solutions\\object_counter.py:174\u001b[39m, in \u001b[36mObjectCounter.process\u001b[39m\u001b[34m(self, im0)\u001b[39m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28mself\u001b[39m.initialize_region()\n\u001b[32m    172\u001b[39m     \u001b[38;5;28mself\u001b[39m.region_initialized = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_tracks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Extract tracks\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;28mself\u001b[39m.annotator = SolutionAnnotator(im0, line_width=\u001b[38;5;28mself\u001b[39m.line_width)  \u001b[38;5;66;03m# Initialize annotator\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;28mself\u001b[39m.annotator.draw_region(\n\u001b[32m    178\u001b[39m     reg_pts=\u001b[38;5;28mself\u001b[39m.region, color=(\u001b[32m104\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m123\u001b[39m), thickness=\u001b[38;5;28mself\u001b[39m.line_width * \u001b[32m2\u001b[39m\n\u001b[32m    179\u001b[39m )  \u001b[38;5;66;03m# Draw region\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Source\\iot-dataanalysis-2025\\mlvenv\\Lib\\site-packages\\ultralytics\\solutions\\solutions.py:120\u001b[39m, in \u001b[36mBaseSolution.extract_tracks\u001b[39m\u001b[34m(self, im0)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_tracks\u001b[39m(\u001b[38;5;28mself\u001b[39m, im0):\n\u001b[32m    109\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[33;03m    Applies object tracking and extracts tracks from an input image or frame.\u001b[39;00m\n\u001b[32m    111\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    118\u001b[39m \u001b[33;03m        >>> solution.extract_tracks(frame)\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28mself\u001b[39m.tracks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43mim0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_add_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m.track_data = \u001b[38;5;28mself\u001b[39m.tracks[\u001b[32m0\u001b[39m].obb \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tracks[\u001b[32m0\u001b[39m].boxes  \u001b[38;5;66;03m# Extract tracks for OBB or object detection\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m.masks = (\n\u001b[32m    124\u001b[39m         \u001b[38;5;28mself\u001b[39m.tracks[\u001b[32m0\u001b[39m].masks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.tracks[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mmasks\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tracks[\u001b[32m0\u001b[39m].masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    125\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Source\\iot-dataanalysis-2025\\mlvenv\\Lib\\site-packages\\ultralytics\\engine\\model.py:588\u001b[39m, in \u001b[36mModel.track\u001b[39m\u001b[34m(self, source, stream, persist, **kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[33;03mConducts object tracking on the specified input source using the registered trackers.\u001b[39;00m\n\u001b[32m    561\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    585\u001b[39m \u001b[33;03m    - Batch size is set to 1 for tracking in videos.\u001b[39;00m\n\u001b[32m    586\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mtrackers\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrackers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_tracker\n\u001b[32m    590\u001b[39m     register_tracker(\u001b[38;5;28mself\u001b[39m, persist)\n\u001b[32m    591\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mconf\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mconf\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0.1\u001b[39m  \u001b[38;5;66;03m# ByteTrack-based method needs low confidence predictions as input\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Source\\iot-dataanalysis-2025\\mlvenv\\Lib\\site-packages\\ultralytics\\trackers\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbot_sort\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BOTSORT\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbyte_tracker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BYTETracker\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_tracker\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Source\\iot-dataanalysis-2025\\mlvenv\\Lib\\site-packages\\ultralytics\\trackers\\bot_sort.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbasetrack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrackState\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbyte_tracker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BYTETracker, STrack\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m matching\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgmc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GMC\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Source\\iot-dataanalysis-2025\\mlvenv\\Lib\\site-packages\\ultralytics\\trackers\\byte_tracker.py:8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m xywh2ltwh\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbasetrack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseTrack, TrackState\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m matching\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkalman_filter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KalmanFilterXYAH\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSTrack\u001b[39;00m(BaseTrack):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Source\\iot-dataanalysis-2025\\mlvenv\\Lib\\site-packages\\ultralytics\\trackers\\utils\\matching.py:17\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchecks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_requirements\n\u001b[32m     16\u001b[39m     check_requirements(\u001b[33m\"\u001b[39m\u001b[33mlap>=0.5.12\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# https://github.com/gatagat/lap\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlap\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlinear_assignment\u001b[39m(cost_matrix: np.ndarray, thresh: \u001b[38;5;28mfloat\u001b[39m, use_lap: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> \u001b[38;5;28mtuple\u001b[39m:\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    Perform linear assignment using either the scipy or lap.lapjv method.\u001b[39;00m\n\u001b[32m     23\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m \u001b[33;03m        >>> matched_indices, unmatched_a, unmatched_b = linear_assignment(cost_matrix, thresh, use_lap=True)\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lap'"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), 'Error reading video file'   # 파일 열리지 않으면 경고처리\n",
    "\n",
    "region_point = [(20, 400), (1080, 400)]     # 라인수\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # 동영상 FPS(Frame Per Second)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))      # 1280\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))    # 720\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(count_path, fourcc, fps, (width, height))\n",
    "\n",
    "# 물체 카운트 초기화\n",
    "counter = ObjectCounter(\n",
    "    show=True,              # 처리하는 동안 디스플레이 여부\n",
    "    region=region_point,     # 라인 위치\n",
    "    model='yolo11n.pt',   # YOLO11 모델\n",
    "    # classes=[0, 2],\n",
    "    # tracker='botsort.yaml'\n",
    ")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    \n",
    "    results = counter(frame)\n",
    "    out.write(results.plot_im)\n",
    "\n",
    "cap.release() # 비디오를 해제\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
